{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d27c1da",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f49dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Foo\") \\\n",
    "    .config(\"spark.jars\",\n",
    "            \"/home/ralfs/Desktop/jars/hudi-spark3-bundle_2.12-0.10.0.1.jar,\" + \n",
    "            \"/home/ralfs/Desktop/jars/clusterj-rondb-21.04.3.jar,\" + \n",
    "            \"/home/ralfs/Desktop/jars/mysql-connector-java-8.0.28.jar\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b76749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark\n",
    "thesispath = \"/home/ralfs/Desktop/temp_dir/\"\n",
    "head_node = \"127.0.0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdd4838",
   "metadata": {},
   "source": [
    "## Setup spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41106246",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "tableName = \"hudi_trips_cow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d68910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "basePath = thesispath + tableName\n",
    "datapath = thesispath + \"data\"\n",
    "testpath = thesispath + \"test\"\n",
    "\n",
    "file_size = 125829120\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09ef1d",
   "metadata": {},
   "source": [
    "## Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11573af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rondb and hbase\n",
    "table = database = \"hudi\"\n",
    "\n",
    "def add_to_dict(dict1, dict2):\n",
    "    for key, value in dict1.items():\n",
    "        if key not in dict2:\n",
    "            dict2[key] = value\n",
    "\n",
    "hudi_options = {\n",
    "    'hoodie.table.name': tableName,\n",
    "    #'hoodie.datasource.write.table.type': 'MERGE_ON_READ',\n",
    "    'hoodie.datasource.write.table.type': 'COPY_ON_WRITE',\n",
    "    'hoodie.datasource.write.recordkey.field': 'uuid',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n",
    "    'hoodie.datasource.write.table.name': tableName,\n",
    "    'hoodie.datasource.write.precombine.field': 'ts',\n",
    "    'hoodie.upsert.shuffle.parallelism': 2,\n",
    "    'hoodie.insert.shuffle.parallelism': 2,\n",
    "    'hoodie.delete.shuffle.parallelism': 2,\n",
    "    #'write.parquet.max.file.size': 1 #Default Value: 120 (MB)\n",
    "    'hoodie.parquet.max.file.size': file_size #Default Value: 125829120 (125829120 Bytes = 120 Megabytes)\n",
    "    #'hoodie.cleaner.commits.retained': 0\n",
    "}\n",
    "\n",
    "hudi_insert_options = {\n",
    "    'hoodie.datasource.write.operation': 'insert'\n",
    "}\n",
    "add_to_dict(hudi_options, hudi_insert_options)\n",
    "\n",
    "hudi_upsert_options = {\n",
    "    'hoodie.datasource.write.operation': 'upsert'\n",
    "}\n",
    "add_to_dict(hudi_options, hudi_upsert_options)\n",
    "\n",
    "hudi_delete_options = {\n",
    "    'hoodie.datasource.write.operation': 'delete'\n",
    "}\n",
    "add_to_dict(hudi_options, hudi_delete_options)\n",
    "\n",
    "# Bloom\n",
    "\n",
    "bloom_options = {\n",
    "    'hoodie.index.type': 'GLOBAL_BLOOM',\n",
    "    'hoodie.bloom.index.update.partition.path': True\n",
    "}\n",
    "bloom_insert_options = dict(bloom_options)\n",
    "add_to_dict(hudi_insert_options, bloom_insert_options)\n",
    "bloom_upsert_options = dict(bloom_options)\n",
    "add_to_dict(hudi_upsert_options, bloom_upsert_options)\n",
    "bloom_delete_options = dict(bloom_options)\n",
    "add_to_dict(hudi_delete_options, bloom_delete_options)\n",
    "\n",
    "# Simple\n",
    "\n",
    "simple_options = {\n",
    "    'hoodie.index.type': 'GLOBAL_SIMPLE',\n",
    "    'hoodie.simple.index.update.partition.path': True\n",
    "}\n",
    "simple_insert_options = dict(simple_options)\n",
    "add_to_dict(hudi_insert_options, simple_insert_options)\n",
    "simple_upsert_options = dict(simple_options)\n",
    "add_to_dict(hudi_upsert_options, simple_upsert_options)\n",
    "simple_delete_options = dict(simple_options)\n",
    "add_to_dict(hudi_delete_options, simple_delete_options)\n",
    "\n",
    "# HBase\n",
    "\n",
    "hbase_options = {\n",
    "    'hoodie.index.type': 'HBASE',\n",
    "    'hoodie.index.hbase.get.batch.size': batch_size,\n",
    "    'hoodie.index.hbase.put.batch.size': batch_size,\n",
    "    'hoodie.index.hbase.max.qps.per.region.server': 1000,\n",
    "    'hoodie.index.hbase.zkquorum': head_node,\n",
    "    'hoodie.index.hbase.zkport': '2181',\n",
    "    'hoodie.index.hbase.table': table,\n",
    "    'hoodie.index.hbase.qps.allocator.class': 'org.apache.hudi.index.hbase.DefaultHBaseQPSResourceAllocator',\n",
    "    'hoodie.hbase.index.update.partition.path': True\n",
    "}\n",
    "hbase_insert_options = dict(hbase_options)\n",
    "add_to_dict(hudi_insert_options, hbase_insert_options)\n",
    "hbase_upsert_options = dict(hbase_options)\n",
    "add_to_dict(hudi_upsert_options, hbase_upsert_options)\n",
    "hbase_delete_options = dict(hbase_options)\n",
    "add_to_dict(hudi_delete_options, hbase_delete_options)\n",
    "\n",
    "# RonDB\n",
    "\n",
    "rondb_options = {\n",
    "    'hoodie.index.type': 'RONDB',\n",
    "    'hoodie.index.rondb.batch.size': batch_size,\n",
    "    'hoodie.index.rondb.update.partition.path': True\n",
    "}\n",
    "rondb_insert_options = dict(rondb_options)\n",
    "add_to_dict(hudi_insert_options, rondb_insert_options)\n",
    "rondb_upsert_options = dict(rondb_options)\n",
    "add_to_dict(hudi_upsert_options, rondb_upsert_options)\n",
    "rondb_delete_options = dict(rondb_options)\n",
    "add_to_dict(hudi_delete_options, rondb_delete_options)\n",
    "\n",
    "# RonDB Cluster\n",
    "\n",
    "rondb_cluster_options = {\n",
    "    'hoodie.index.type': 'RONDB_CLUSTER',\n",
    "    'hoodie.index.rondb.batch.size': batch_size,\n",
    "    'hoodie.index.rondb.clusterj': {\n",
    "        'com.mysql.clusterj.connectstring': '127.0.0.1:1186',\n",
    "        'com.mysql.clusterj.database': database\n",
    "    },\n",
    "    'hoodie.index.rondb.update.partition.path': True\n",
    "}\n",
    "rondb_cluster_insert_options = dict(rondb_cluster_options)\n",
    "add_to_dict(hudi_insert_options, rondb_cluster_insert_options)\n",
    "rondb_cluster_upsert_options = dict(rondb_cluster_options)\n",
    "add_to_dict(hudi_upsert_options, rondb_cluster_upsert_options)\n",
    "rondb_cluster_delete_options = dict(rondb_cluster_options)\n",
    "add_to_dict(hudi_delete_options, rondb_cluster_delete_options)\n",
    "\n",
    "# All options\n",
    "\n",
    "options_dict = {\n",
    "    'bloom': {\n",
    "        'insert': bloom_insert_options,\n",
    "        'upsert': bloom_upsert_options,\n",
    "        'delete': bloom_delete_options\n",
    "    },\n",
    "    'simple': {\n",
    "        'insert': simple_insert_options,\n",
    "        'upsert': simple_upsert_options,\n",
    "        'delete': simple_delete_options\n",
    "    },\n",
    "    'hbase': {\n",
    "        'insert': hbase_insert_options,\n",
    "        'upsert': hbase_upsert_options,\n",
    "        'delete': hbase_delete_options\n",
    "    },\n",
    "    'rondb': {\n",
    "        'insert': rondb_insert_options,\n",
    "        'upsert': rondb_upsert_options,\n",
    "        'delete': rondb_delete_options\n",
    "    },\n",
    "    'rondb_cluster': {\n",
    "        'insert': rondb_cluster_insert_options,\n",
    "        'upsert': rondb_cluster_upsert_options,\n",
    "        'delete': rondb_cluster_delete_options\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d546ce",
   "metadata": {},
   "source": [
    "## DB stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ba192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import happybase\n",
    "import pymysql.cursors\n",
    "\n",
    "def reset_hbase():\n",
    "    connection = happybase.Connection()\n",
    "\n",
    "    connection.disable_table(table)\n",
    "    connection.delete_table(table)\n",
    "    \n",
    "    connection.create_table(\n",
    "        table,\n",
    "        {\n",
    "            '_s': dict() #the column family used by Hudi\n",
    "        }\n",
    "    )\n",
    "    print(\"hbase reset\")\n",
    "\n",
    "reset_hbase()\n",
    "    \n",
    "def reset_rondb():\n",
    "    connection = pymysql.connect(\n",
    "        host=head_node,\n",
    "        port=3306,\n",
    "        user='root',\n",
    "        passwd=''\n",
    "    )\n",
    "    \n",
    "    mycursor = connection.cursor()\n",
    "    mycursor.execute(\"DROP DATABASE IF EXISTS hudi;\")\n",
    "\n",
    "    connection.commit()\n",
    "    print(\"rondb reset\")\n",
    "\n",
    "reset_rondb()\n",
    "\n",
    "def reset_db():\n",
    "    reset_hbase()\n",
    "    reset_rondb()\n",
    "    \n",
    "reset_rondb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e3039",
   "metadata": {},
   "source": [
    "## Create entries used in tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac75189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_insert = 'insert'\n",
    "action_upsert = 'upsert'\n",
    "action_delete = 'delete'\n",
    "\n",
    "action_types = [action_insert, action_upsert, action_delete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "def display_info(text):\n",
    "    clear_output(wait=True)\n",
    "    display(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5bf5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def init_data(num_of_entries, dataGen, batch_size = 50000):\n",
    "    first = True\n",
    "    inserted_entries = 0\n",
    "\n",
    "    while True: # done in batches to avoid java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
    "        display_info(inserted_entries)\n",
    "        \n",
    "        insert_num = min(batch_size, num_of_entries - inserted_entries)\n",
    "        if insert_num <= 0:\n",
    "            break\n",
    "        data = dataGen.generateInserts(insert_num)\n",
    "        data_list = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(data)\n",
    "        df = spark.read.json(spark.sparkContext.parallelize(data_list, 2))\n",
    "        \n",
    "        if first:\n",
    "            first = False\n",
    "            df.write.mode(\"overwrite\").parquet(datapath)\n",
    "        else:\n",
    "            df.write.mode(\"append\").parquet(datapath)\n",
    "        inserted_entries += insert_num\n",
    "\n",
    "#init_data(1000000, 50000)\n",
    "\n",
    "def init_tests(action_type, num_of_tests, num_of_changes_in_test, dataGen, test_info_df):\n",
    "    for test_nr in range(num_of_tests):\n",
    "        df = None\n",
    "        if action_type == action_insert:        \n",
    "            data = dataGen.generateInserts(num_of_changes_in_test)\n",
    "            data_list = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(data)\n",
    "            df = spark.read.json(spark.sparkContext.parallelize(data_list, 2))\n",
    "        elif action_type == action_upsert:        \n",
    "            data = dataGen.generateUpdates(num_of_changes_in_test)\n",
    "            data_list = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(data)\n",
    "            df = spark.read.json(spark.sparkContext.parallelize(data_list, 2))\n",
    "            df = df.withColumn(\"partitionpath\", lit(\"space/space/space\")) # to update path\n",
    "            df.head()\n",
    "        else:\n",
    "            data = dataGen.generateUpdates(num_of_changes_in_test)\n",
    "            data_list = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(data)\n",
    "            df = spark.read.json(spark.sparkContext.parallelize(data_list, 2))\n",
    "\n",
    "        file_name = f\"{action_type}_{test_nr}\"\n",
    "        df.write.mode(\"overwrite\").parquet(os.path.join(testpath, file_name))\n",
    "        test_info = {\n",
    "                'action': action_type,\n",
    "                'run_nr': test_nr,\n",
    "                'file_name': file_name}\n",
    "        test_info_df = test_info_df.append(test_info, ignore_index = True)\n",
    "    return test_info_df\n",
    "\n",
    "#init_tests(\"insert\")\n",
    "\n",
    "def setup(num_of_entries, num_of_tests, num_of_changes_in_test):\n",
    "    dataGen = sc._jvm.org.apache.hudi.QuickstartUtils.DataGenerator()\n",
    "    \n",
    "    #data\n",
    "    init_data(num_of_entries, dataGen)\n",
    "    \n",
    "    #test\n",
    "    test_info_df = pd.DataFrame(columns = ['action', 'run_nr', 'file_name'])\n",
    "    for action_type in action_types:\n",
    "        print(f\"    {action_type}\")\n",
    "        test_info_df = init_tests(action_type, num_of_tests, num_of_changes_in_test, dataGen, test_info_df)\n",
    "\n",
    "    dataGen = None\n",
    "    \n",
    "    test_info_path = os.path.join(testpath, \"test_info.csv\")\n",
    "    if os.path.exists(test_info_path):\n",
    "        os.remove(test_info_path)\n",
    "    \n",
    "    test_info_df.to_csv(test_info_path, index=False) # saves test info\n",
    "\n",
    "#setup(5)\n",
    "\n",
    "def bootstrap_hudi(options):\n",
    "    #uses prev df saved in datapath and options to initialize hudi table\n",
    "    start_time = time.time()\n",
    "    df = spark.read.format(\"parquet\").load(datapath)\n",
    "    options['hoodie.index.hbase.get.batch.size'] = 500\n",
    "    options['hoodie.index.hbase.put.batch.size'] = 500\n",
    "    df.write.format(\"hudi\").options(**options).mode(\"overwrite\").save(basePath)\n",
    "    print(f\"bootstrap took: {time.time()-start_time}\")\n",
    "    options['hoodie.index.hbase.get.batch.size'] = batch_size\n",
    "    options['hoodie.index.hbase.put.batch.size'] = batch_size\n",
    "    \n",
    "    #sleep_sec = 10\n",
    "    #print(f\"now sleeping for: {sleep_sec}\")\n",
    "    #time.sleep(sleep_sec) # sleeping to remove impact from the bootstrap\n",
    "\n",
    "#bootstrap_hudi(bloom_upsert_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b31c623",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ecb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(df, options):\n",
    "    start_time = time.time()\n",
    "    df.write.format(\"hudi\").options(**options).mode(\"append\").save(basePath)\n",
    "    end_time = time.time()\n",
    "\n",
    "    return end_time - start_time\n",
    "\n",
    "def test_implementation(implementation_name, test_info_df):\n",
    "    times_list = []\n",
    "    for index, row in test_info_df.iterrows():\n",
    "        test_df = spark.read.format(\"parquet\").load(os.path.join(testpath, row['file_name']))\n",
    "\n",
    "        result_time = run_test(test_df, options_dict[implementation_name][row['action']])\n",
    "\n",
    "        print(f\"{row['action']}: {row['run_nr']} --- {result_time} seconds ---\")\n",
    "        times_list.append(result_time)\n",
    "    return times_list\n",
    "\n",
    "\n",
    "def test_implementations():\n",
    "    #reset_path(resultspath) # resets results (remember to seve and rename after running)\n",
    "    \n",
    "    test_info_df = pd.read_csv(os.path.join(testpath, 'test_info.csv'))\n",
    "    \n",
    "    #options_dict\n",
    "    for implementation_name in options_dict:\n",
    "        print(implementation_name)\n",
    "        \n",
    "        reset_db()\n",
    "        bootstrap_hudi(options_dict[implementation_name][\"upsert\"]) # always upsert\n",
    "        print(f\"{implementation_name}: bootstraped\")\n",
    "        \n",
    "        times_list = test_implementation(implementation_name, test_info_df)\n",
    "        test_info_df[implementation_name] = times_list\n",
    "    \n",
    "    reset_db()\n",
    "    \n",
    "    results_path = os.path.join(thesispath, \"results.csv\")\n",
    "    if os.path.exists(results_path):\n",
    "        os.remove(results_path)\n",
    "    \n",
    "    test_info_df.to_csv(results_path, index=False) # saves results\n",
    "\n",
    "#test_implementation('bloom') # no db\n",
    "#test_implementation('simple') # no db\n",
    "#test_implementation('hbase') # hbase running\n",
    "#test_implementation('rondb', pd.DataFrame()) # rondb running\n",
    "#test_implementation('rondb_cluster') # rondb running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b8825e",
   "metadata": {},
   "source": [
    "## Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbb9b32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# have to run seperatly and make sure db are working only for the current index\n",
    "\n",
    "setup(1000000, 10, 100) # run 1 time before all implementations\n",
    "test_implementations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165722ff",
   "metadata": {},
   "source": [
    "full transparancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9291022",
   "metadata": {},
   "source": [
    "ideas\n",
    "can see roolback times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d734c245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
